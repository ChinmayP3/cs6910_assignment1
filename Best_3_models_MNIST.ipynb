{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNNlw7Z-S965"
      },
      "source": [
        "#Assignment 1: Build FeedForward Neural Network Architecture \n",
        "\n",
        "The goal of this assignment is twofold: (i) implement and use gradient descent (and its variants) with\n",
        "backpropagation for a classification task (ii) get familiar with wandb which is a cool tool for running and keeping track of a large number of experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4QoUJr0THu3"
      },
      "source": [
        "### Import required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_V3jTIQpTOyS"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import fashion_mnist\n",
        "from sklearn import metrics\n",
        "from keras.datasets import mnist\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt \n",
        "import matplotlib.colors\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error\n",
        "from sklearn.metrics import log_loss\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        " \n",
        " \n",
        " \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6_p5F27TRDy"
      },
      "source": [
        "### Installing and Login to Wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "kE3b2hyRN6tC",
        "outputId": "6ffb7795-ceb1-4773-9b8b-9489b19ff240"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.1/189.1 KB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 KB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "!pip install wandb -qqq\n",
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayQLhGJoAwQv",
        "outputId": "827c21a7-f45d-4270-d373-829ac2bc1c4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "['T-shirt/Top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot']\n"
          ]
        }
      ],
      "source": [
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "labels = [\"T-shirt/Top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle Boot\"]\n",
        "print(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-uSN_ZDMAw_q",
        "outputId": "7681e38e-9668-4ec5-9a92-032e708839d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n",
            "60000\n",
            "60000\n",
            "[5 0 4 ... 5 6 8]\n"
          ]
        }
      ],
      "source": [
        "# checking details of dataset\n",
        "print(train_images.shape)\n",
        "print(len(train_images))\n",
        "print(len(train_labels))\n",
        "print(train_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dhbdkXYBA_bT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6h8FtxYBAAB",
        "outputId": "2faf5f7f-8d7b-4ef5-c1e5-34075db8ae84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "60000 28 28\n"
          ]
        }
      ],
      "source": [
        "N , a , b = train_images.shape\n",
        "#Dimesion of each Datapoint will be a*b\n",
        "print(N,a,b)\n",
        "x = a*b\n",
        "d = x\n",
        "nl = len(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNmFx17BbCY9",
        "outputId": "4e9d7a6c-163b-4add-df39-d259f9bec0ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "labels = [\"T-shirt/Top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle Boot\"]\n",
        "N , a , b = train_images.shape\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "_uzxa_ShHd7U",
        "outputId": "8792cb5d-54f0-4ed0-e67c-e62b51d1ad99"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs22m035\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.14.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230319_124042-0yugplp2</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/cs22m035/Assignment%201/runs/0yugplp2' target=\"_blank\">Question 1</a></strong> to <a href='https://wandb.ai/cs22m035/Assignment%201' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/cs22m035/Assignment%201' target=\"_blank\">https://wandb.ai/cs22m035/Assignment%201</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/cs22m035/Assignment%201/runs/0yugplp2' target=\"_blank\">https://wandb.ai/cs22m035/Assignment%201/runs/0yugplp2</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/cs22m035/Assignment%201/runs/0yugplp2?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7f4fabbafb50>"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wandb.init(project=\"Assignment 1\",name=\"Question 1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l39O5G0LLouw"
      },
      "source": [
        "#Question 1\n",
        "Download the fashion-MNIST dataset and plot 1 sample image for each class as shown in the grid below. Use from keras.datasets import fashion_mnist for getting the fashion mnist dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "1DDfpoyEVbND"
      },
      "outputs": [],
      "source": [
        "def show_images(train_images , train_labels , desc):\n",
        "  \n",
        "    fig, axs = plt.subplots(2, 5, figsize=(12, 6))\n",
        "    axs = axs.flatten()\n",
        "    class_images=[]\n",
        "\n",
        "    for i in range(10):\n",
        "          index=np.argmax(train_labels==i)\n",
        "          # Plot the image\n",
        "          axs[i].imshow(train_images[index], cmap='gray')\n",
        "          axs[i].set_title(labels[i])\n",
        "          axs[i].axis('off')\n",
        "          image = wandb.Image(train_images[index], caption=labels[i])\n",
        "          class_images.append(image)\n",
        "\n",
        "    wandb.log({\"examples\": class_images})\n",
        "    \n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIDREPHNHRWh"
      },
      "source": [
        "# Question 2 (10 Marks)\n",
        "Implement a feedforward neural network which takes images from the fashion-mnist data as input and outputs a probability distribution over the 10 classes.\n",
        "\n",
        "Your code should be flexible such that it is easy to change the number of hidden layers and the number of neurons in each hidden layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkCkYjcLLusM"
      },
      "source": [
        "###Propecessing of data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJ2nvR4fdIdz",
        "outputId": "21b150c7-479f-4a14-c8b1-a83e468e0bb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n",
            "Shape of flattened_train_images is :  (60000, 784)\n",
            "Shape of flattened_test_images is  :  (10000, 784)\n"
          ]
        }
      ],
      "source": [
        "#Flattening we are converting the 28*28 data point as 784*1 data point\n",
        "print(train_images.shape)\n",
        "\n",
        "flattened_images = []\n",
        "for i in range(len(train_images)):\n",
        "    flattened_image = train_images[i].flatten()\n",
        "    flattened_images.append(flattened_image)\n",
        "flatted_train_images = np.array(flattened_images)\n",
        "\n",
        "print(\"Shape of flattened_train_images is : \",flatted_train_images.shape)\n",
        "\n",
        "  \n",
        "#Same transition for testdata\n",
        "flattened_images = []\n",
        "for i in range(len(test_images)):\n",
        "    flattened_image = test_images[i].flatten()\n",
        "    flattened_images.append(flattened_image)\n",
        "flatted_test_images = np.array(flattened_images)\n",
        "print(\"Shape of flattened_test_images is  : \",flatted_test_images.shape)\n",
        "     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "uC7nrfY8WEWB"
      },
      "outputs": [],
      "source": [
        "#getting train data and validation data from training data\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_x, valid_x, cat_train_y, cat_val_y = train_test_split(flatted_train_images, train_labels, test_size=0.1, stratify = train_labels ,random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOnDidLOiujh",
        "outputId": "613109f8-41c6-4f62-98f9-34a3dd2447ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(54000, 10)\n",
            "(6000, 10)\n",
            "(10000, 10)\n"
          ]
        }
      ],
      "source": [
        "#One hot encoding of categorical labels \n",
        "def encode(data, nl):\n",
        "    encode_data = np.zeros((len(data), nl))\n",
        "    for i in range(len(data)):\n",
        "        actual_label = data[i]\n",
        "        encode_data[i][actual_label] = 1\n",
        "    return encode_data\n",
        "\n",
        "\n",
        "#One hot ecoding \n",
        "train_y = encode(cat_train_y , nl)\n",
        "print(train_y.shape)\n",
        "valid_y = encode(cat_val_y , nl)\n",
        "print(valid_y.shape)\n",
        "test_y = encode(test_labels , nl)\n",
        "print(test_y.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "4gB5g1rbiuoh"
      },
      "outputs": [],
      "source": [
        "#Normalizing the data\n",
        "mean = np.mean(train_x , axis = 0)\n",
        "train_x = (train_x - mean) /255\n",
        "test_x = (flatted_test_images - mean)/255\n",
        "valid_x = (valid_x - mean)/255\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "nr5kCu-plvfi"
      },
      "outputs": [],
      "source": [
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfc75empL3_4"
      },
      "source": [
        "#Activation functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Nb8lRFqmSSY5"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Sigmoid \n",
        " \n",
        "def sigmoid(x):\n",
        "    temp = []\n",
        "    # exp_xi=[]\n",
        "    for i in range(len(x)):\n",
        "        # exp_xi=np.exp(-float(x[i]))\n",
        "        result = (1/(1 + np.exp(-(float(x[i])))))\n",
        "        temp.append(result)\n",
        "    return np.array(temp)\n",
        "#Tanh\n",
        "def tanh(x):\n",
        "    temp = []\n",
        "    for i in range(len(x)):\n",
        "      result=np.tanh(x[i])\n",
        "      temp.append(result)\n",
        "\n",
        "    return np.array(temp)\n",
        "#ReLU\n",
        "def relu(x):\n",
        "    temp = []\n",
        "    for i in range(len(x)):\n",
        "      if x[i]>0:\n",
        "        temp.append(x[i])\n",
        "      elif x[i]<=0:\n",
        "        temp.append(0)\n",
        "    return np.array(temp)\n",
        "    \n",
        "#Softmax\n",
        "def softmax(x):\n",
        "  \n",
        "  sum=0\n",
        "  for i in range(len(x)):\n",
        "    sum+=np.exp(float(x[i]))\n",
        "  temp=[]  \n",
        "  for i in range(len(x)):\n",
        "    result=np.exp(float(x[i]))\n",
        "    temp.append(result/sum)\n",
        "\n",
        "  return np.array(temp)\n",
        "\n",
        "#Derivative of Sigmoid\n",
        "def derivative_sigmoid(x):\n",
        "    temp=x*(1 - x)\n",
        "    return temp\n",
        "\n",
        "#Derivative of Tanh\n",
        "def derivative_tanh(x):\n",
        "    cal_tanh=1-np.square(x)\n",
        "    return cal_tanh\n",
        "\n",
        "#Derivative of ReLU\n",
        "def derivative_relu(x):\n",
        "    temp = []\n",
        "    sum=0\n",
        "    for i in range(len(x)):\n",
        "      if x[i]<=0:\n",
        "        temp.append(0)\n",
        "      elif x[i]>0:\n",
        "        temp.append(1)\n",
        "    return np.array(temp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJ-tlLaeMLtI"
      },
      "source": [
        "#Network initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "yO6X3QBoMxae"
      },
      "outputs": [],
      "source": [
        " \n",
        "def intitialize_zero_input(dim, hl, ol,w,b,d):\n",
        "\n",
        "    for i in range(len(hl)):\n",
        "        hl_size=hl[i]\n",
        "        b.append(np.zeros(hl_size))\n",
        "       \n",
        "        if i == 0:\n",
        "            \n",
        "            w.append(np.zeros((hl_size, d)))\n",
        "        else:\n",
        "            lst_size=hl[i-1]\n",
        "            w.append(np.zeros((hl_size,lst_size )))\n",
        "    return w,b\n",
        "\n",
        "def initialize_zero_output(im, hl, ol,w,b,d):\n",
        "\n",
        "    for i in range(len(ol)):\n",
        "        ol_size=ol[i]\n",
        "        b.append(np.zeros(ol_size))\n",
        "        w.append(np.zeros((ol_size, hl[-1])))\n",
        "    return w,b\n",
        "\n",
        "\n",
        "def initialize_zeros(dim, hl, ol):\n",
        "    w = [np.array([])]\n",
        "    b = [np.array([])]\n",
        "    d = dim\n",
        "  \n",
        "    w,b=intitialize_zero_input(dim, hl, ol,w,b,d)\n",
        "    w,b=initialize_zero_output(dim, hl, ol,w,b,d)\n",
        "\n",
        "    return w, b\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kq38vwJxHi7u"
      },
      "source": [
        "### Random Initialization And Xavier Initialization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "5Anplb6Dlvh-"
      },
      "outputs": [],
      "source": [
        "def intitialize_random_input(dim, hl, ol,W,b,d):\n",
        "    np.random.seed(2)\n",
        "\n",
        "    for i in range(len(hl)):\n",
        "        hd_size=hl[i]\n",
        "        b.append(np.random.randn(hd_size))\n",
        "        if(i == 0):\n",
        "          W.append(np.random.randn(hd_size,dim))\n",
        "        else:\n",
        "          lst_size=hl[i - 1]\n",
        "          W.append(np.random.randn(hd_size,lst_size))\n",
        "\n",
        "    return W,b\n",
        "\n",
        "def initialize_random_output(im, hl, ol,W,b,d):\n",
        "\n",
        "    np.random.seed(2)\n",
        "    for i in range(len(ol)):\n",
        "        ol_size=ol[i]\n",
        "        lst_hl_size=hl[-1]\n",
        "        b.append(np.random.randn(ol_size))\n",
        "        W.append(np.random.randn(ol_size,lst_hl_size))\n",
        "    return W,b\n",
        "\n",
        "def random_initialization(dim, hl, ol,W,b,d):\n",
        "   \n",
        "  \n",
        "    W,b=intitialize_random_input(dim, hl, ol,W,b,d)\n",
        "    W,b=initialize_random_output(dim, hl, ol,W,b,d)\n",
        "\n",
        "    return W,b\n",
        "\n",
        "def intitialize_xavier_input(dim, hl, ol,W,b,d):\n",
        "    np.random.seed(2)\n",
        "\n",
        "    for i in range(len(hl)):\n",
        "        hl_size=hl[i]\n",
        "        b.append( np.random.randn(hl_size))\n",
        "        if (i == 0):\n",
        "            W.append(np.random.randn(hl_size,dim ) * np.sqrt(1/dim))\n",
        "        else:\n",
        "            lst_hd_size=hl[i-1]\n",
        "            W.append(np.random.randn(hl_size,lst_hd_size) * np.sqrt(1/lst_hd_size))\n",
        "    return W,b     \n",
        "\n",
        "def initialize_xavier_output(im, hl, ol,W,b,d):\n",
        "    np.random.seed(2)\n",
        "    for i in range(len(ol)):\n",
        "        ol_size=ol[i]\n",
        "        b.append(np.random.randn(ol_size))\n",
        "        lst_ol=hl[-1]\n",
        "        W.append(np.random.randn(ol_size, lst_ol)* np.sqrt(1/lst_ol))\n",
        "    return W,b\n",
        "\n",
        "def xavier_initialization(dim, hl, ol,W,b,d):\n",
        "   \n",
        "  \n",
        "    W,b=intitialize_xavier_input(dim, hl, ol,W,b,d)\n",
        "    W,b=initialize_xavier_output(dim, hl, ol,W,b,d)\n",
        "\n",
        "    return  W,b\n",
        "\n",
        "#Initialize Network \n",
        "def initialize_network(dim, hl, ol, method):\n",
        "  np.random.seed(2)\n",
        "  # Declaring 2d numpy array\n",
        "  W = [np.array([])]\n",
        "  b = [np.array([])]\n",
        " \n",
        "  d = dim\n",
        "  #Setting up the random seed\n",
        "  \n",
        "\n",
        "  #Random Intialization\n",
        "  if(method=='random'):\n",
        "     W,b=random_initialization(dim,hl,ol,W,b,d)\n",
        "      \n",
        "  #Xavier Initialization\n",
        "  else:\n",
        "     W,b= xavier_initialization(dim,hl,ol,W,b,d)\n",
        "\n",
        "  return W,b\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Y7VQGKpMQXO"
      },
      "source": [
        "#Forward Propagation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "yHRXzM_ysJua"
      },
      "outputs": [],
      "source": [
        " \n",
        "\n",
        "def sigmoid_method(w,b,x,a,c,d,h):\n",
        "  num_layers = (len(w)-1)\n",
        "  for i in range(1 , num_layers):\n",
        "            wh=np.dot( w[i], h[i-1] )\n",
        "            c = wh + b[i]\n",
        "            a.append(c)\n",
        "            d = sigmoid(c)\n",
        "            h.append(d)\n",
        "def tanh_method(w,b,x,a,c,d,h):\n",
        "  num_layers = (len(w)-1)\n",
        "  for i in range(1 , num_layers):\n",
        "            c = (w[i]@h[i-1])+b[i]\n",
        "            a.append(c)\n",
        "            h.append(tanh(c))\n",
        "\n",
        "\n",
        "def relu_method(w,b,x,a,c,d,h):\n",
        "  num_layers = (len(w)-1)\n",
        "  for i in range(1 , num_layers):\n",
        "            c = (w[i]@h[i-1])+b[i]            \n",
        "            a.append(c)\n",
        "            h.append(relu(c))\n",
        "\n",
        "#Forward Propagation Framework\n",
        "def forward_propagation(w,b,x,method='sigmoid'):\n",
        "    a = [[]]\n",
        "    h = [[]]\n",
        "    # inserting data into first layer\n",
        "    h[0] = x\n",
        "    hl_size=(len(w)-1)\n",
        "    num_layers = hl_size\n",
        "    c = []\n",
        "    d = []\n",
        "    #Sigmoid as activation function in every hidden layer\n",
        "    if method=='sigmoid':\n",
        "        sigmoid_method(w,b,x,a,c,d,h)\n",
        "    #Tanh as activation function in every hidden layer\n",
        "    elif method=='tanh':\n",
        "        tanh_method(w,b,x,a,c,d,h)\n",
        "    #ReLU as activation function in every hidden layer\n",
        "    elif method=='relu':\n",
        "       relu_method(w,b,x,a,c,d,h)\n",
        "    #Softmax at output Layer\n",
        "    c = w[num_layers] @ h[num_layers-1] + b[num_layers]\n",
        "    d = softmax(c)\n",
        "    a.append(c)\n",
        "    h.append(d)\n",
        "    \n",
        "\n",
        "    return a,h"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-l1OXNypMUSf"
      },
      "source": [
        "#Backward Propagation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "fTdgFIC4sbnI"
      },
      "outputs": [],
      "source": [
        " \n",
        "# Loss function cross entropy\n",
        "def cross_entropy_loss_fun(y_pred,y):\n",
        "  cr=y_pred-y\n",
        "  return cr\n",
        "\n",
        "# Loss squared error\n",
        "def sqrd_error_loss_fun(y_pred,y):\n",
        "  y_label = y_pred[np.argmax(y)]\n",
        "  res= 2 * (y_label - 1) * y_label * ( y - y_pred )\n",
        "  return res\n",
        "\n",
        " \n",
        "#Backpropagation Framework\n",
        "def back_prop(W,h,x,y,y_pred,act_fun,loss_fun):\n",
        "  del_W=[[]]\n",
        "  del_b=[[]]\n",
        "\n",
        "  #Computing output grad wrt Cross Entropy Loss function\n",
        "  if loss_fun == \"cross_entropy\" :\n",
        " \n",
        "    del_a = cross_entropy_loss_fun(y_pred,y)\n",
        "    \n",
        "    \n",
        "  #Computing output grad wrt Squared Error Loss function\n",
        "  else:\n",
        " \n",
        "    del_a=sqrd_error_loss_fun(y_pred,y)\n",
        "\n",
        "  for i in range(len(W)-1, 0, -1):\n",
        "\n",
        "    #computing gradients wrt parameters W,b\n",
        "    \n",
        "    db = np.array( del_a )\n",
        "    dot_prod = np.dot(np.matrix(del_a).T , np.matrix(h[i-1]))\n",
        "    dW = np.array(dot_prod)\n",
        "    \n",
        "\n",
        "    #computing gradients wrt below layer activation function\n",
        "    dh = np.dot( np.transpose(W[i]), del_a )\n",
        "\n",
        "    #computing gradients wrt below layer pre-activation function\n",
        "    if act_fun == \"sigmoid\":\n",
        "      del_a = dh *  h[i-1] * (1 - h[i-1])\n",
        "    \n",
        "    elif act_fun == \"tanh\":\n",
        "      del_a = dh * (1 - h[i-1]**2)\n",
        "    \n",
        "    elif act_fun == \"relu\":\n",
        "      del_a = dh * (h[i-1] > 0)\n",
        "\n",
        "    del_W.insert(1, dW)\n",
        "    del_b.insert(1, db)\n",
        "\n",
        "  return del_W, del_b\n",
        "\n",
        " \n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Im9PeUfFHseQ"
      },
      "source": [
        "# Question 3 (24 Marks)\n",
        "Implement the backpropagation algorithm with support for the following optimisation functions\n",
        "\n",
        "* sgd\n",
        "* momentum based gradient descent\n",
        "* nesterov accelerated gradient descent\n",
        "* rmsprop\n",
        "* adam\n",
        "* nadam\n",
        "\n",
        "(12 marks for the backpropagation framework and 2 marks for each of the optimisation algorithms above)\n",
        "\n",
        "We will check the code for implementation and ease of use (e.g., how easy it is to add a new optimisation algorithm such as Eve). Note that the code should be flexible enough to work with different batch sizes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXprOTMuMaQ1"
      },
      "source": [
        "#Stochastic Gradient Descent optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_8yKZRTH0_0"
      },
      "source": [
        "#### Printing errors and Logging data in Wandb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "-A44hpFluYZP"
      },
      "outputs": [],
      "source": [
        "def print_error_log_wandb(e,train_acc,val_acc,test_acc,train_loss,val_loss,loss_fun):\n",
        "\n",
        "    print(\"epoch:\" , e+1 , \" \"+loss_fun+\" \", \"train_acc :\" , train_acc , \"valid_acc :\" , val_acc , \"test_acc :\" , test_acc)\n",
        "    \n",
        "    if loss_fun==\"cross_entropy\":\n",
        "      wandb.log({\n",
        "          \"Epoch\": e+1,\n",
        "          \"Train Loss\": train_loss,\n",
        "          \"Train Acc\": train_acc,\n",
        "          \"Valid Loss\": val_loss,\n",
        "          \"Valid Acc\": val_acc})\n",
        "    else:\n",
        "      wandb.log({\n",
        "          \"Train Loss (squared_error)\": train_loss,\n",
        "          \"Train Acc (squared_error)\": train_acc,\n",
        "          \"Valid Loss (squared_error)\": val_loss,\n",
        "          \"Valid Acc (squared_error)\": val_acc})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "b4B_B7zxsbpx"
      },
      "outputs": [],
      "source": [
        " \n",
        "\n",
        "def update_dw_db(tdw,tdb,dw,db):\n",
        "    for i in range(len(tdw)):\n",
        "        dw[i] += tdw[i]\n",
        "        db[i] += tdb[i]\n",
        "    return dw,db\n",
        "   \n",
        "def stochastic_gradient_descent(train_x,train_y,valid_x,valid_y,d,hl,ol,act_fun,loss_fun,epochs,eta,strat,alpha,batch):\n",
        "  size_image=784\n",
        "  #Initializing the weights and biases based on given strategy\n",
        "  w,b = initialize_network(d, hl, ol, strat)\n",
        "  counter=0\n",
        " \n",
        "  dw , db = initialize_zeros(size_image,hl,ol)\n",
        "\n",
        "  seen = 0\n",
        "  for e in range(epochs):\n",
        "    counter+=1\n",
        "    for it, (x, y) in enumerate(zip(train_x, train_y)):\n",
        "\n",
        "      a,h = forward_propagation(w, b, x, act_fun)\n",
        "      last_ele=len(h)-1\n",
        "      y_pred=h[last_ele]\n",
        "      seen += 1\n",
        "     \n",
        "      tdw,tdb=back_prop(w, h, x, y, y_pred, act_fun, loss_fun)\n",
        "\n",
        "      dw,db=update_dw_db(tdw,tdb,dw,db)\n",
        "\n",
        "      if(seen==batch or it==len(train_x)-1):\n",
        "        seen = 0\n",
        "        for i in range(len(w)):\n",
        "            weight=w[i]\n",
        "            deriv=dw[i]\n",
        "            w[i] = weight - eta * np.array(deriv)\n",
        "\n",
        "        for i in range(len(b)):\n",
        "            bias=b[i]\n",
        "            deriv=db[i]\n",
        "            b[i] = bias - eta * np.array(deriv)\n",
        "        \n",
        "        dw , db = initialize_zeros(784,hl,ol)\n",
        "\n",
        "    #Getting train,val,test accuracies and losses and predictions with true labels\n",
        "    val_acc, val_loss,yt_val,ypred_train = get_predictions_accuracy(w, b, valid_x, valid_y, act_fun, loss_fun)\n",
        "    train_acc, train_loss,yt_train,ypred_train = get_predictions_accuracy(w, b, train_x, train_y, act_fun, loss_fun)\n",
        "    test_acc, test_loss,yt_test,ypred_test = get_predictions_accuracy(w, b, test_x, test_y, act_fun, loss_fun)\n",
        "\n",
        "    print_error_log_wandb(e,train_acc,val_acc,test_acc,train_loss,val_loss,loss_fun)   \n",
        "  return w,b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7JpBnmAMhNU"
      },
      "source": [
        "#Momentum Based Gradient Descent "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "wYT8Ed0eVIwu"
      },
      "outputs": [],
      "source": [
        " \n",
        "\n",
        "\n",
        " \n",
        "def momentum_gradient_descent(train_x,train_y,valid_x,valid_y,d,hl,ol,act_fun,loss_fun,epochs,eta,strat,alpha,batch):\n",
        "  gamma=0.9\n",
        "  seen=0\n",
        "  W,b=initialize_network(d, hl, ol, strat)\n",
        "  gamma=0.9\n",
        "  seen=0\n",
        "  counter=0\n",
        "  dw , db = initialize_zeros(784,hl,ol)\n",
        "  prev_w , prev_b = initialize_zeros(784,hl,ol)\n",
        "  for e in range(epochs):\n",
        "    counter+=1\n",
        "    for it, (x, y) in enumerate(zip(train_x, train_y)):\n",
        "      seen+=1\n",
        "      a,h = forward_propagation(W, b, x, act_fun)\n",
        "      y_pred=h[len(h)-1]\n",
        "      \n",
        "      tdw,tdb=back_prop(W, h, x, y, y_pred, act_fun, loss_fun)\n",
        "  \n",
        "      dw,db=update_dw_db(tdw,tdb,dw,db)\n",
        "    \n",
        "      if(seen==batch or it==len(train_x)-1):\n",
        "        seen=0\n",
        "        for i in range(len(W)):\n",
        "            weight = W[i]\n",
        "            deriv = dw[i]\n",
        "            new_weight = weight - eta * np.array(deriv) - gamma * prev_w[i]\n",
        "            new_prev_w = eta * np.array(deriv) + gamma * prev_w[i]\n",
        "            W[i] = new_weight\n",
        "            prev_w[i] = new_prev_w\n",
        "\n",
        "\n",
        "        for i in range(len(b)):\n",
        "            bias = b[i]\n",
        "            deriv = db[i]\n",
        "            new_bias = bias - eta * np.array(deriv) - gamma * prev_b[i]\n",
        "            new_prev_b = eta * np.array(deriv) + gamma * prev_b[i]\n",
        "            b[i] = new_bias\n",
        "            prev_b[i] = new_prev_b\n",
        "            \n",
        "            dw , db = initialize_zeros(784,hl,ol)\n",
        "\n",
        "    val_acc, val_loss,yt_val,ypred_train = get_predictions_accuracy(W, b, valid_x, valid_y, act_fun, loss_fun)\n",
        "    train_acc, train_loss,yt_train,ypred_train = get_predictions_accuracy(W, b, train_x, train_y, act_fun, loss_fun)\n",
        "    test_acc, test_loss,yt_test,ypred_test = get_predictions_accuracy(W, b, test_x, test_y, act_fun, loss_fun)\n",
        "\n",
        "    print_error_log_wandb(e,train_acc,val_acc,test_acc,train_loss,val_loss,loss_fun)\n",
        "\n",
        "  return W,b"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Ybb-fTh2MlKf"
      },
      "source": [
        "#Nesterov Accelerated Gradient Descent\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "WOFtP3b3r6lo"
      },
      "outputs": [],
      "source": [
        " \n",
        "def cal_v_w_v_b(W,v_w,v_b,gamma,prev_w,prev_b):\n",
        "    for i in range(len(W)):\n",
        "        v_w[i] = gamma*prev_w[i]\n",
        "        v_b[i] = gamma*prev_b[i]\n",
        "    return v_w,v_b\n",
        "    \n",
        "def cal_t_w_t_b(W,tw,tb,v_w,v_b,b):\n",
        "    for i in range(len(W)):\n",
        "        tw[i] = W[i] - v_w[i]\n",
        "        tb[i] = b[i] - v_b[i]\n",
        "    return tw,tb\n",
        "\n",
        "def nesterov_gradient_descent(train_x,train_y,valid_x,valid_y,d,hl,ol,act_fun,loss_fun,epochs,eta,strat,alpha,batch):\n",
        "  W,b=initialize_network(d, hl, ol, strat)\n",
        "\n",
        "  gamma=0.9\n",
        "  seen=0\n",
        "  \n",
        "  prev_w , prev_b = initialize_zeros(784,hl,ol)\n",
        "  v_w , v_b =initialize_zeros(784,hl,ol)\n",
        "\n",
        "  dw , db = initialize_zeros(784,hl,ol)\n",
        "  tw , tb = initialize_zeros(784,hl,ol)\n",
        "\n",
        "  for e in range(epochs):\n",
        "\n",
        "     \n",
        "    v_w,v_b = cal_v_w_v_b(W,v_w,v_b,gamma,prev_w,prev_b)\n",
        "    \n",
        "    tw,tb = cal_t_w_t_b(W,tw,tb,v_w,v_b,b)\n",
        "     \n",
        "    for it, (x, y) in enumerate(zip(train_x, train_y)):\n",
        "      a,h = forward_propagation(tw, tb, x, act_fun)\n",
        "      hl_size=len(h)-1\n",
        "      y_pred=h[hl_size]\n",
        "      seen+=1\n",
        "      tdw,tdb=back_prop(tw, h, x, y, y_pred, act_fun, loss_fun)\n",
        "      \n",
        "      update_dw_db(tdw,tdb,dw,db)\n",
        "\n",
        "      if seen==batch or it == len(train_x)-1:\n",
        "        seen=0\n",
        "        for i, (weight, deriv) in enumerate(zip(W, dw)):\n",
        "          v_w[i] = gamma*prev_w[i] + eta*np.array(deriv)\n",
        "          W[i] = weight - v_w[i]\n",
        "          tw[i] = W[i]\n",
        "          prev_w = v_w\n",
        "\n",
        "        for i, (bias, deriv) in enumerate(zip(b, db)):\n",
        "          v_b[i] = gamma*prev_b[i] + eta*np.array(deriv)\n",
        "          b[i] = bias - v_b[i]\n",
        "          tb[i] = b[i]\n",
        "          prev_b = v_b\n",
        "\n",
        "\n",
        "        dw , db = initialize_zeros(784,hl,ol)\n",
        "\n",
        "    val_acc, val_loss,yt_val,ypred_train = get_predictions_accuracy(W, b, valid_x, valid_y, act_fun, loss_fun)\n",
        "    train_acc, train_loss,yt_train,ypred_train = get_predictions_accuracy(W, b, train_x, train_y, act_fun, loss_fun)\n",
        "    test_acc, test_loss,yt_test,ypred_test = get_predictions_accuracy(W, b, test_x, test_y, act_fun, loss_fun)\n",
        "\n",
        "    print_error_log_wandb(e,train_acc,val_acc,test_acc,train_loss,val_loss,loss_fun)\n",
        "\n",
        "  return W,b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRsDnJLyM7xm"
      },
      "source": [
        "#RMS_Prop Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "iCF8hfEfZrRo"
      },
      "outputs": [],
      "source": [
        "def rmsprop(train_x,train_y,valid_x,valid_y,d,hl,ol,act_fun,loss_fun,epochs,eta,strat,alpha,batch):\n",
        "  prev_w , prev_b = initialize_zeros(784,hl,ol)\n",
        "  eps = 1e-8 \n",
        "  beta = 0.9\n",
        "  W,b=initialize_network(d, hl, ol, strat)\n",
        "  seen=0\n",
        "  \n",
        "  prev_w , prev_b = initialize_zeros(784,hl,ol)\n",
        "  dw , db = initialize_zeros(784,hl,ol)\n",
        "   \n",
        "\n",
        "  for e in range(epochs):\n",
        "  \n",
        "    for it, (x, y) in enumerate(zip(train_x, train_y)):\n",
        "      \n",
        "      a,h = forward_propagation(W, b, x, act_fun)\n",
        "      hl_size=len(h)-1\n",
        "      y_pred=h[hl_size]\n",
        "      seen+=1\n",
        "      tdw,tdb=back_prop(W, h, x, y, y_pred, act_fun, loss_fun)\n",
        "      \n",
        "      dw,db=update_dw_db(tdw,tdb,dw,db)\n",
        "      \n",
        "      if seen==batch or it == len(train_x)-1:\n",
        "\n",
        "        seen=0\n",
        "        for i, (dw_i, db_i) in enumerate(zip(dw,db)):\n",
        "          prev_w[i] = beta*prev_w[i] + (1-beta)*(dw_i**2)\n",
        "          prev_b[i] = beta*prev_b[i] + (1-beta)*(db_i**2)\n",
        "\n",
        "        for i in range(len(W)):\n",
        "          weight=W[i]\n",
        "          deriv=dw[i]\n",
        "          W[i] = weight - (eta / np.sqrt(prev_w[i] + eps)) * np.array(deriv)\n",
        "\n",
        "        for i in range(len(W)):\n",
        "          bias=b[i]\n",
        "          deriv=db[i]\n",
        "          b[i] = bias - (eta / np.sqrt(prev_b[i] + eps)) * np.array(deriv)\n",
        "        \n",
        "        dw , db = initialize_zeros(784,hl,ol)\n",
        "\n",
        "    val_acc, val_loss,yt_val,ypred_train = get_predictions_accuracy(W, b, valid_x, valid_y, act_fun, loss_fun)\n",
        "    train_acc, train_loss,yt_train,ypred_train = get_predictions_accuracy(W, b, train_x, train_y, act_fun, loss_fun)\n",
        "    test_acc, test_loss,yt_test,ypred_test = get_predictions_accuracy(W, b, test_x, test_y, act_fun, loss_fun)\n",
        "\n",
        "    print_error_log_wandb(e,train_acc,val_acc,test_acc,train_loss,val_loss,loss_fun)\n",
        "  return W,b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIlcCzcJNOdq"
      },
      "source": [
        "#Adam Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "qpjZJAhk47Sy"
      },
      "outputs": [],
      "source": [
        "def adaptive_moments(train_x,train_y,valid_x,valid_y,d,hl,ol,act_fun,loss_fun,epochs,eta,strat,alpha,batch):\n",
        "  dimention=784\n",
        "  dw , db = initialize_zeros(dimention,hl,ol)\n",
        "  m_w , m_b = initialize_zeros(dimention,hl,ol)\n",
        "  eps=1e-8\n",
        "  v_w , v_b = initialize_zeros(dimention,hl,ol)\n",
        "  beta1 , beta2 =  0.9 , 0.99\n",
        "  W,b=initialize_network(d, hl, ol, strat)\n",
        "  m_w_hat , m_b_hat = initialize_zeros(dimention,hl,ol)\n",
        "  v_w_hat , v_b_hat = initialize_zeros(dimention,hl,ol)\n",
        "  seen=0\n",
        "  c=0\n",
        "\n",
        "  for e in range(epochs):\n",
        "  \n",
        "    for it, (x, y) in enumerate(zip(train_x, train_y)):\n",
        "      \n",
        "      a,h = forward_propagation(W, b, x, act_fun)\n",
        "      hl_size=len(h)-1\n",
        "      y_pred=h[hl_size]\n",
        "      seen+=1\n",
        "\n",
        "      tdw,tdb=back_prop(W, h, x, y, y_pred, act_fun, loss_fun)\n",
        "      \n",
        "      dw,db=update_dw_db(tdw,tdb,dw,db)\n",
        "\n",
        "      if seen==batch or it == len(train_x)-1:\n",
        "\n",
        "        seen=0\n",
        "        c+=1\n",
        "        for i in range(len(W)):\n",
        "          dw_i=dw[i]\n",
        "          db_i=db[i]\n",
        "          m_w[i] = beta1 * m_w[i] + (1 - beta1) * dw_i\n",
        "          m_b[i] = beta1 * m_b[i] + (1 - beta1) * db_i\n",
        "          v_w[i] = beta2 * v_w[i] + (1 - beta2) * dw_i ** 2\n",
        "          v_b[i] = beta2 * v_b[i] + (1 - beta2) * db_i ** 2\n",
        "\n",
        "\n",
        "        for i in range(len(W)):\n",
        "          denom1=(1 - np.power(beta1,c))\n",
        "          denom2=(1 - np.power(beta2,c))\n",
        "\n",
        "          m_w_hat[i] = m_w[i] / denom1\n",
        "          m_b_hat[i] = m_b[i] / denom1\n",
        "\n",
        "          v_w_hat[i] = v_w[i] / denom2\n",
        "          v_b_hat[i] = v_b[i] / denom2\n",
        "\n",
        "        for i in range(len(W)):\n",
        "          weight=W[i]\n",
        "          deriv=dw[i]\n",
        "          W[i] = weight - (eta / np.sqrt(v_w_hat[i] + eps)) * m_w_hat[i]\n",
        "\n",
        "        for i in range(len(W)):\n",
        "          bias=b[i]\n",
        "          deriv=db[i]\n",
        "          b[i] = bias - (eta / np.sqrt(v_b_hat[i] + eps)) * m_b_hat[i]\n",
        "        \n",
        "        dw , db = initialize_zeros(784,hl,ol)\n",
        "\n",
        "    val_acc, val_loss,yt_val,ypred_train = get_predictions_accuracy(W, b, valid_x, valid_y, act_fun, loss_fun)\n",
        "    train_acc, train_loss,yt_train,ypred_train = get_predictions_accuracy(W, b, train_x, train_y, act_fun, loss_fun)\n",
        "    test_acc, test_loss,yt_test,ypred_test = get_predictions_accuracy(W, b, test_x, test_y, act_fun, loss_fun)\n",
        "\n",
        "    print_error_log_wandb(e,train_acc,val_acc,test_acc,train_loss,val_loss,loss_fun)\n",
        "\n",
        "  return W,b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfDrDkl0NTfM"
      },
      "source": [
        "#Nadam Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "dScKlrVAMjHu"
      },
      "outputs": [],
      "source": [
        "def nadam(train_x,train_y,valid_x,valid_y,d,hl,ol,act_fun,loss_fun,epochs,eta,strat,alpha,batch):\n",
        "  dimention=784\n",
        "  dw , db = initialize_zeros(dimention,hl,ol)\n",
        "  m_w , m_b = initialize_zeros(dimention,hl,ol)\n",
        "  eps=1e-8\n",
        "  v_w , v_b = initialize_zeros(dimention,hl,ol)\n",
        "  beta1 , beta2 =  0.9 , 0.99\n",
        "  W,b=initialize_network(d, hl, ol, strat)\n",
        "  m_w_hat , m_b_hat = initialize_zeros(dimention,hl,ol)\n",
        "  v_w_hat , v_b_hat = initialize_zeros(dimention,hl,ol)\n",
        "  seen=0\n",
        "  c=0\n",
        "\n",
        "  for e in range(epochs):\n",
        "  \n",
        "    for it, (x, y) in enumerate(zip(train_x, train_y)):\n",
        "      \n",
        "      a,h = forward_propagation(W, b, x, act_fun)\n",
        "      y_pred=h[len(h)-1]\n",
        "      \n",
        "      tdw,tdb=back_prop(W, h, x, y, y_pred, act_fun, loss_fun)\n",
        "      \n",
        "      dw,db=update_dw_db(tdw,tdb,dw,db)\n",
        "      seen+=1\n",
        "      if seen==batch or it == len(train_x)-1:\n",
        "\n",
        "        seen=0\n",
        "        c+=1\n",
        "        for i, (dw_i, db_i) in enumerate(zip(dw, db)):\n",
        "          m_w[i] = beta1 * m_w[i] + (1-beta1) * dw_i\n",
        "          m_b[i] = beta1 * m_b[i] + (1-beta1) * db_i\n",
        "\n",
        "          v_w[i] = beta2 * v_w[i] + (1-beta2) * (dw_i**2)\n",
        "          v_b[i] = beta2 * v_b[i] + (1-beta2) * (db_i**2)\n",
        "\n",
        "\n",
        "        for i in range(len(W)):\n",
        "          denom1=(1 - np.power(beta1,c))\n",
        "          denom2=(1 - np.power(beta2,c))\n",
        "\n",
        "          m_w_hat[i] = ( (beta1 * m_w[i]) + ( (1 - beta1) * dw[i] ) )/ denom1\n",
        "          m_b_hat[i] = ( (beta1 * m_b[i]) + ( (1 - beta1) * db[i] ) )/ denom1\n",
        "\n",
        "          v_w_hat[i] = v_w[i] / denom2\n",
        "          v_b_hat[i] = v_b[i] / denom2\n",
        "\n",
        "        for i in range(len(W)):\n",
        "          weight=W[i]\n",
        "          deriv=dw[i]\n",
        "          W[i] = weight - (eta / np.sqrt(v_w_hat[i] + eps)) * m_w_hat[i]\n",
        "\n",
        "        for i in range(len(b)):\n",
        "          bias=b[i]\n",
        "          deriv=db[i]\n",
        "          b[i] = bias - (eta / np.sqrt(v_b_hat[i] + eps)) * m_b_hat[i]\n",
        "        \n",
        "        dw , db = initialize_zeros(784,hl,ol)\n",
        "\n",
        "    val_acc, val_loss,yt_val,ypred_train = get_predictions_accuracy(W, b, valid_x, valid_y, act_fun, loss_fun)\n",
        "    train_acc, train_loss,yt_train,ypred_train = get_predictions_accuracy(W, b, train_x, train_y, act_fun, loss_fun)\n",
        "    test_acc, test_loss,yt_test,ypred_test = get_predictions_accuracy(W, b, test_x, test_y, act_fun, loss_fun)\n",
        "\n",
        "    print_error_log_wandb(e,train_acc,val_acc,test_acc,train_loss,val_loss,loss_fun)\n",
        "\n",
        "  return W,b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-lW4pgQIZag"
      },
      "source": [
        "#Confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "0SW5mfI0JWAp"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.figure_factory as ff\n",
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "def get_confusion_matrix(y_true , y_pred , class_names , figsize=(20,20)):\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=class_names)\n",
        "    fontsize = 14\n",
        "    r=class_names[::-1]\n",
        "    #z_text = [[str(y_true) for y_true in y_pred] for y_pred in cm]\n",
        "\n",
        "    df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
        "\n",
        "    cm_sum = np.sum(cm, axis=1, keepdims=True)\n",
        "    cm_perc = cm / cm_sum.astype(float) * 100\n",
        "    annot = np.empty_like(cm).astype(str)\n",
        "    h = [ [0 for _ in range(len(class_names))] for _ in range(len(class_names)) ]\n",
        "    #h = np.empty_like(cm).astype(str)\n",
        "    nrows, ncols = cm.shape\n",
        "    for i in range(1,nrows+1):\n",
        "        for j in range(1,ncols+1):\n",
        "            c = cm[i-1, j-1]\n",
        "            p = cm_perc[i-1, j-1]\n",
        "            if i == j:\n",
        "                s = cm_sum[i-1]\n",
        "                annot[i-1, j-1] = '%.1f%% <br> %d/%d' % (p, c, s)\n",
        "                h[i-1][j-1] = '%d %s are correctly classified' % (c,labels[i-1])\n",
        "            elif c == 0:\n",
        "                annot[i-1, j-1] = ''\n",
        "                h[i-1][j-1] = ''\n",
        "            else:\n",
        "                annot[i-1, j-1] = '%.1f%% <br> %d' % (p, c)\n",
        "                h[i-1][j-1] = '%d %s are wrongly classified as %s' % (c,labels[i-1],labels[j-1])\n",
        "\n",
        "\n",
        "    fig = ff.create_annotated_heatmap(cm , x = class_names, y = class_names , text=h,annotation_text = annot , hoverinfo='text' ,colorscale ='blugrn_R')\n",
        "    fig['layout']['yaxis']['autorange'] = \"reversed\"\n",
        "    \n",
        "   \n",
        "    # add custom xaxis title\n",
        "    fig.add_annotation(dict(font=dict(color=\"black\",size=14),x=0.5,y=-0.15,showarrow=False,text=\"Predicted value\",xref=\"paper\",yref=\"paper\"))\n",
        "\n",
        "    # add custom yaxis title\n",
        "    fig.add_annotation(dict(font=dict(color=\"black\",size=14),x=-0.35,y=0.5,showarrow=False,text=\"Real value\",textangle=-90,xref=\"paper\",yref=\"paper\"))\n",
        "\n",
        "    # adjust margins to make room for yaxis title\n",
        "    fig.update_layout(margin=dict(t=50, l=200))\n",
        "    fig.update_annotations(font_size=14)\n",
        "   \n",
        "    fig['data'][0]['showscale'] = True\n",
        "    fig.update_xaxes(side=\"top\")\n",
        "    fig.show()\n",
        "    return fig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myOelQjrNW2f"
      },
      "source": [
        "#Predictions and accuracy function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "ULyXvX_Qr8Sh"
      },
      "outputs": [],
      "source": [
        "def predict_sigmoid(a,h,W, b, X, y, method, loss_fun,num_layers):\n",
        "    for i in range(1 , num_layers):\n",
        "        a = np.dot( W[i], h ) + b[i]\n",
        "        h = sigmoid(a)\n",
        "    return a,h\n",
        "\n",
        "def predict_tanh(a,h,W, b, X, y, method, loss_fun,num_layers):\n",
        "    for i in range(1 , num_layers):\n",
        "        a = np.dot( W[i], h ) + b[i]\n",
        "        h = tanh(a)\n",
        "    return a,h\n",
        "\n",
        "def predict_relu(a,h,W, b, X, y, method, loss_fun,num_layers):\n",
        "    for i in range(1 , num_layers):\n",
        "        a = np.dot( W[i], h ) + b[i]\n",
        "        h = relu(a)\n",
        "    return a,h\n",
        "\n",
        "def get_predictions_accuracy(W, b, X, y, method, loss_fun):\n",
        "  sum,loss=0,0\n",
        "  yhat = []\n",
        "  yt = []\n",
        "  for dp in range(len(X)):\n",
        "    a = []\n",
        "    h = []\n",
        "    h = X[dp] \n",
        "    num_layers = (len(W)-1)\n",
        "\n",
        "    if method=='sigmoid':\n",
        "        a,h = predict_sigmoid(a,h,W, b, X, y, method, loss_fun,num_layers)\n",
        "\n",
        "    elif method=='tanh':\n",
        "        a,h = predict_tanh(a,h,W, b, X, y, method, loss_fun,num_layers)\n",
        "\n",
        "    elif method=='relu':\n",
        "        a,h = predict_relu(a,h,W, b, X, y, method, loss_fun,num_layers)\n",
        "\n",
        "    a = np.dot( W[num_layers], h ) + b[num_layers]\n",
        "    y_pred = softmax(a)\n",
        "\n",
        "    ytrue = y[dp]\n",
        "    if(ytrue[np.argmax(y_pred)]==1):\n",
        "      sum=sum+1\n",
        "    \n",
        "    if loss_fun == \"cross_entropy\":\n",
        "      loss += -np.sum(ytrue*np.log(y_pred))\n",
        "    else:\n",
        "      loss += np.sum((ytrue-y_pred)**2)\n",
        "\n",
        "    yhat.append(labels[np.argmax(y_pred)])\n",
        "    yt.append(labels[np.argmax(ytrue)])\n",
        "\n",
        "  acc=sum/len(X)\n",
        "  loss=loss/len(X)\n",
        "\n",
        "  return acc,loss,yt,yhat\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQDfvTCyTlt-"
      },
      "source": [
        "\n",
        "# Best Model 1 on MNIST\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkma8-3RqQ-_",
        "outputId": "28e63eff-808c-4d58-ddfd-4f933f0d31d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 1  cross_entropy  train_acc : 0.8728518518518519 valid_acc : 0.8701666666666666 test_acc : 0.8542\n",
            "epoch: 2  cross_entropy  train_acc : 0.8882222222222222 valid_acc : 0.882 test_acc : 0.8638\n",
            "epoch: 3  cross_entropy  train_acc : 0.897425925925926 valid_acc : 0.8853333333333333 test_acc : 0.8679\n",
            "epoch: 4  cross_entropy  train_acc : 0.9002407407407408 valid_acc : 0.8853333333333333 test_acc : 0.8692\n",
            "epoch: 5  cross_entropy  train_acc : 0.9061296296296296 valid_acc : 0.8873333333333333 test_acc : 0.872\n",
            "epoch: 6  cross_entropy  train_acc : 0.914425925925926 valid_acc : 0.8906666666666667 test_acc : 0.8761\n",
            "epoch: 7  cross_entropy  train_acc : 0.9129814814814815 valid_acc : 0.8878333333333334 test_acc : 0.8727\n",
            "epoch: 8  cross_entropy  train_acc : 0.9199444444444445 valid_acc : 0.8895 test_acc : 0.8746\n",
            "epoch: 9  cross_entropy  train_acc : 0.9188148148148149 valid_acc : 0.8865 test_acc : 0.8715\n",
            "epoch: 10  cross_entropy  train_acc : 0.924537037037037 valid_acc : 0.8911666666666667 test_acc : 0.8768\n"
          ]
        }
      ],
      "source": [
        "optimizer=nadam\n",
        "epochs=10\n",
        "batch = 64\n",
        "eta=0.001\n",
        "alpha=0\n",
        "hidden_layer_size = 128\n",
        "hidden_layers = 5\n",
        "strat='xavier'\n",
        "act_fun ='relu'\n",
        "loss_fun='cross_entropy'\n",
        "hl = [hidden_layer_size]*hidden_layers\n",
        "ol = [len(train_y[0])]\n",
        "n_hl = len(hl)\n",
        "\n",
        "if optimizer==stochastic_gradient_descent:\n",
        "  W,b = stochastic_gradient_descent(train_x,train_y,valid_x,valid_y,d,hl,ol,act_fun,loss_fun,epochs,eta,strat,alpha,batch)\n",
        "\n",
        "elif optimizer==momentum_gradient_descent:\n",
        "  W,b = momentum_gradient_descent(train_x,train_y,valid_x,valid_y,d,hl,ol,act_fun,loss_fun,epochs,eta,strat,alpha,batch)\n",
        "\n",
        "elif optimizer==nesterov_gradient_descent:\n",
        "  W,b = nesterov_gradient_descent(train_x,train_y,valid_x,valid_y,d,hl,ol,act_fun,loss_fun,epochs,eta,strat,alpha,batch)\n",
        "\n",
        "elif optimizer==rmsprop:\n",
        "  W,b = rmsprop(train_x,train_y,valid_x,valid_y,d,hl,ol,act_fun,loss_fun,epochs,eta,strat,alpha,batch)\n",
        "\n",
        "elif optimizer==adaptive_moments:\n",
        "  W,b = adaptive_moments(train_x,train_y,valid_x,valid_y,d,hl,ol,act_fun,loss_fun,epochs,eta,strat,alpha,batch)\n",
        "\n",
        "elif optimizer==nadam:\n",
        "  W,b = nadam(train_x,train_y,valid_x,valid_y,d,hl,ol,act_fun,loss_fun,epochs,eta,strat,alpha,batch)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBxz6NPyOEgM"
      },
      "source": [
        "# Best Model 2 on MNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fiT72DwmvD-c",
        "outputId": "dbc49de1-d0e5-4dd9-cc53-ea5c49df2517"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 1  cross_entropy  train_acc : 0.8685555555555555 valid_acc : 0.8701666666666666 test_acc : 0.8533\n",
            "epoch: 2  cross_entropy  train_acc : 0.8849444444444444 valid_acc : 0.8833333333333333 test_acc : 0.8621\n"
          ]
        }
      ],
      "source": [
        "optimizer=adaptive_moments\n",
        "epochs=10\n",
        "batch = 64\n",
        "eta=0.001\n",
        "alpha=0\n",
        "hidden_layer_size = 128\n",
        "hidden_layers = 5\n",
        "strat='xavier'\n",
        "act_fun ='relu'\n",
        "loss_fun='cross_entropy'\n",
        "hl = [hidden_layer_size]*hidden_layers\n",
        "ol = [len(train_y[0])]\n",
        "n_hl = len(hl)\n",
        "\n",
        "if optimizer==stochastic_gradient_descent:\n",
        "  W,b = stochastic_gradient_descent(train_x,train_y,valid_x,valid_y,d,hl,ol,act_fun,loss_fun,epochs,eta,strat,alpha,batch)\n",
        "\n",
        "elif optimizer==momentum_gradient_descent:\n",
        "  W,b = momentum_gradient_descent(train_x,train_y,valid_x,valid_y,d,hl,ol,act_fun,loss_fun,epochs,eta,strat,alpha,batch)\n",
        "\n",
        "elif optimizer==nesterov_gradient_descent:\n",
        "  W,b = nesterov_gradient_descent(train_x,train_y,valid_x,valid_y,d,hl,ol,act_fun,loss_fun,epochs,eta,strat,alpha,batch)\n",
        "\n",
        "elif optimizer==rmsprop:\n",
        "  W,b = rmsprop(train_x,train_y,valid_x,valid_y,d,hl,ol,act_fun,loss_fun,epochs,eta,strat,alpha,batch)\n",
        "\n",
        "elif optimizer==adaptive_moments:\n",
        "  W,b = adaptive_moments(train_x,train_y,valid_x,valid_y,d,hl,ol,act_fun,loss_fun,epochs,eta,strat,alpha,batch)\n",
        "\n",
        "elif optimizer==nadam:\n",
        "  W,b = nadam(train_x,train_y,valid_x,valid_y,d,hl,ol,act_fun,loss_fun,epochs,eta,strat,alpha,batch)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjbIWmncOJTk"
      },
      "source": [
        "# Best Model 3 on MNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4OPrOA_8vED1"
      },
      "outputs": [],
      "source": [
        "optimizer=rmsprop\n",
        "epochs=5\n",
        "batch = 32\n",
        "eta=0.001\n",
        "alpha=0\n",
        "hidden_layer_size = 128\n",
        "hidden_layers = 5\n",
        "strat='xavier'\n",
        "act_fun ='relu'\n",
        "loss_fun='cross_entropy'\n",
        "hl = [hidden_layer_size]*hidden_layers\n",
        "ol = [len(train_y[0])]\n",
        "n_hl = len(hl)\n",
        "\n",
        "if optimizer==stochastic_gradient_descent:\n",
        "  W,b = stochastic_gradient_descent(train_x,train_y,valid_x,valid_y,d,hl,ol,act_fun,loss_fun,epochs,eta,strat,alpha,batch)\n",
        "\n",
        "elif optimizer==momentum_gradient_descent:\n",
        "  W,b = momentum_gradient_descent(train_x,train_y,valid_x,valid_y,d,hl,ol,act_fun,loss_fun,epochs,eta,strat,alpha,batch)\n",
        "\n",
        "elif optimizer==nesterov_gradient_descent:\n",
        "  W,b = nesterov_gradient_descent(train_x,train_y,valid_x,valid_y,d,hl,ol,act_fun,loss_fun,epochs,eta,strat,alpha,batch)\n",
        "\n",
        "elif optimizer==rmsprop:\n",
        "  W,b = rmsprop(train_x,train_y,valid_x,valid_y,d,hl,ol,act_fun,loss_fun,epochs,eta,strat,alpha,batch)\n",
        "\n",
        "elif optimizer==adaptive_moments:\n",
        "  W,b = adaptive_moments(train_x,train_y,valid_x,valid_y,d,hl,ol,act_fun,loss_fun,epochs,eta,strat,alpha,batch)\n",
        "\n",
        "elif optimizer==nadam:\n",
        "  W,b = nadam(train_x,train_y,valid_x,valid_y,d,hl,ol,act_fun,loss_fun,epochs,eta,strat,alpha,batch)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdlm_S-Q0iO1"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ceIkf_3u0g95"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaYRj2Y42H5B"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQodczbf1r6v"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wtd2Z3fZ1r91"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-a5s06G1sBn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqdQZxpv224F"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6pyJnOnB0hEE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ICK8eMTr0hIe"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8bgYO4E0hK3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KLtQSg7yfJb"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4mOs8xeyh95"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uh1Gdmp2kTTh"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uxM0fyvukbIB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0umqZPs3hAgT"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smOJIEaZgsyM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ceh9Q2AgocG"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSc8sVZQNw9B"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPo--5Bhj3Ik"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bC4eZDwh2hip"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9SchTUV2hlh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SDvE0Ejd2hoG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XjduC7Va2hqg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wDnquUG7pPCn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tfa5A0xrA2NC"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
